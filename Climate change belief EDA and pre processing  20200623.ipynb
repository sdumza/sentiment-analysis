{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nptBt84ztT1C"
   },
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/project-name.png\" align = \"left\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/team-name.png\" width = \"40%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.unsplash.com/photo-1569060368645-4ab30c8d8b0e?ixlib=rb-1.2.1&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1920\" width = \"60%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/overview.png\" width = \"30%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "2. Import Packages and Data\n",
    "3. Data Exploration\n",
    "4. Data Preprocessing\n",
    "5. Model Building\n",
    "6. Model Prediction\n",
    "7. Conclusion\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZvO1Ab_tT1M"
   },
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/introduction.png\" width = \"40%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAeCWjqJtT1N"
   },
   "source": [
    "## Problem Defintion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLl161R6V3N8"
   },
   "source": [
    "## Problem Statement\n",
    "To determine the belief in climate change threat by building a (machine learning) classifier and performing sentiment analysis based off tweets to assist market researchers gauge how their products may be received by the consumers.\n",
    "\n",
    "#### Why\n",
    "This will give companies built around lessening one’s environmental impact or carbon footprint access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
    "\n",
    "#### How\n",
    "By Analysing text data from tweets posted about climate change and predicting the sentiment that people have about climate change\n",
    "\n",
    "#### What\n",
    "A Machine Learning model deployed on a web application that is able to classify their sentiment about climate change, based on their novel tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCqzIvY-tT1R"
   },
   "source": [
    "## Problem Landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgIuYeeYWLzl"
   },
   "source": [
    "\n",
    "### Data\n",
    "* Open-source Tweet Data\n",
    "* Data from twitter made available on Kaggle\n",
    "* structured\n",
    "\n",
    "#### Information\n",
    "1. Classification Machine Learning\n",
    "2. Climate Change\n",
    "\n",
    "\n",
    "#### Knowledge\n",
    "[... something... ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCnmUTkHtT1T"
   },
   "source": [
    "## Equation of Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXc48d8GWeW2"
   },
   "source": [
    "\n",
    "$NC = $New Clients + Customer Churn ($v$)\n",
    "\n",
    "$Ppp = $ Price per Product\n",
    "\n",
    "$Score_{ML} = $ A measure of the machine learning model's predictive ability - typically (F1 score, accuracy or precision from the _Confusion Matrix_). For this project the **F1 Score** will be used.\n",
    "\n",
    "\n",
    "$$ NC = v(Score_{ML})$$\n",
    "\n",
    "$$ Revenue = f(Ppp, NC) $$\n",
    "\n",
    "\n",
    "The machine learning model can add value by helping companies identify which areas will lead to more sales.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGmmaLMdtT1V"
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/import-packages.png\" width = \"30%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following code anywhere in your machine learning file\n",
    "experiment = comet_ml.Experiment(api_key=\"quY9CXKJTLd4wCLNuIQqCuVGa\",\n",
    "                        project_name=\"joziradicals-jhb-ss5-classification\",\n",
    "                        workspace=\"tiroamodimo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting comet_ml\n",
      "  Downloading comet_ml-3.1.12-py2.py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (7.352.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (2.0.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (2.23.0)\n",
      "Collecting comet-git-pure>=0.19.11\n",
      "  Downloading comet_git_pure-0.19.16-py3-none-any.whl (409 kB)\n",
      "\u001b[K     |████████████████████████████████| 409 kB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting netifaces>=0.10.7\n",
      "  Downloading netifaces-0.10.9.tar.gz (28 kB)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (3.2.0)\n",
      "Requirement already satisfied: websocket-client>=0.55.0 in /opt/conda/lib/python3.7/site-packages (from comet_ml) (0.57.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from comet_ml) (1.14.0)\n",
      "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\"\n",
      "  Downloading everett-1.0.2-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->comet_ml) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->comet_ml) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.18.4->comet_ml) (2.9)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (1.6.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (46.1.3.post20200325)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (19.3.0)\n",
      "Collecting configobj; extra == \"ini\"\n",
      "  Downloading configobj-5.0.6.tar.gz (33 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=3.1.0,>=2.6.0->comet_ml) (3.1.0)\n",
      "Building wheels for collected packages: netifaces, configobj\n",
      "  Building wheel for netifaces (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for netifaces: filename=netifaces-0.10.9-cp37-cp37m-linux_x86_64.whl size=37295 sha256=0f8b1080b7b1a6108b4f854e96e807bde7072bd88845f38df95144a5b996be90\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/09/cf/2b1aa8371c071fa89518ac0bbda1b8cca4e65b6e2538af4192\n",
      "  Building wheel for configobj (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for configobj: filename=configobj-5.0.6-py3-none-any.whl size=34546 sha256=1115f1e61f3ce7d2b739cb856125c7c6a07292ff1736aeb613f66f9945984902\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/c4/19/13d74440f2a571841db6b6e0a273694327498884dafb9cf978\n",
      "Successfully built netifaces configobj\n",
      "Installing collected packages: comet-git-pure, netifaces, configobj, everett, comet-ml\n",
      "Successfully installed comet-git-pure-0.19.16 comet-ml-3.1.12 configobj-5.0.6 everett-1.0.2 netifaces-0.10.9\n"
     ]
    }
   ],
   "source": [
    "!pip install comet_ml\n",
    "# !pip install nltk\n",
    "# !pip install spacy\n",
    "# !pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "# data analysis libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import string\n",
    "import scipy.sparse\n",
    "\n",
    "# other\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# visualisation libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Notebook styling\n",
    "%matplotlib\n",
    "sns.set()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "#import textblob\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# ML Pre processing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ML Model Building\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# ML Model Testing and Optimisation\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ML Model Deployment\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/import-data.png\" width = \"20%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data files\n",
    "basepath = 'resources/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train and test data files\n",
    "train_df = pd.read_csv(basepath + '/train.csv')\n",
    "test_df = pd.read_csv(basepath + '/test.csv')\n",
    "#sample_submission_df = pd.read_csv(basepath + '/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @darreljorstad: Funny as hell! Canada deman...</td>\n",
       "      <td>897853122080407553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>All the biggest lies about climate change and ...</td>\n",
       "      <td>925046776553529344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>The Coming Revelation Of The $q$Global Warming...</td>\n",
       "      <td>696354236850786305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @DineshDSouza: Let's see if the world ends ...</td>\n",
       "      <td>846806509732483072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @SteveSGoddard: Obama has no control over t...</td>\n",
       "      <td>628085266293653504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  RT @darreljorstad: Funny as hell! Canada deman...   \n",
       "1         -1  All the biggest lies about climate change and ...   \n",
       "2         -1  The Coming Revelation Of The $q$Global Warming...   \n",
       "3         -1  RT @DineshDSouza: Let's see if the world ends ...   \n",
       "4         -1  RT @SteveSGoddard: Obama has no control over t...   \n",
       "\n",
       "              tweetid  \n",
       "0  897853122080407553  \n",
       "1  925046776553529344  \n",
       "2  696354236850786305  \n",
       "3  846806509732483072  \n",
       "4  628085266293653504  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check training data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check test data\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WXBamzOtT1Z"
   },
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/eda-header.png\" width = \"70%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq7x6aEJtT1b"
   },
   "source": [
    "After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense.\n",
    "\n",
    "**Analysis of Words**\n",
    "\n",
    "* Word Count - Total Number of Words\n",
    "* Word Frequency  - find most common and create word clouds\n",
    "* Uncommon Word Count - number of uncommon words\n",
    "* Uncommon Word Frequnce - find least common and create word clouds\n",
    "* Parts of Speech - 8 parts of speech will be analysed\n",
    "\n",
    "**Analysis of Twitter Indicators**\n",
    "* Use of handles\n",
    "    * How many use handles\n",
    "    * Use of frequently used handles - bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @darreljorstad: Funny as hell! Canada deman...</td>\n",
       "      <td>897853122080407553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>All the biggest lies about climate change and ...</td>\n",
       "      <td>925046776553529344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>The Coming Revelation Of The $q$Global Warming...</td>\n",
       "      <td>696354236850786305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @DineshDSouza: Let's see if the world ends ...</td>\n",
       "      <td>846806509732483072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @SteveSGoddard: Obama has no control over t...</td>\n",
       "      <td>628085266293653504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  RT @darreljorstad: Funny as hell! Canada deman...   \n",
       "1         -1  All the biggest lies about climate change and ...   \n",
       "2         -1  The Coming Revelation Of The $q$Global Warming...   \n",
       "3         -1  RT @DineshDSouza: Let's see if the world ends ...   \n",
       "4         -1  RT @SteveSGoddard: Obama has no control over t...   \n",
       "\n",
       "              tweetid  \n",
       "0  897853122080407553  \n",
       "1  925046776553529344  \n",
       "2  696354236850786305  \n",
       "3  846806509732483072  \n",
       "4  628085266293653504  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view training data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view test data\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in train data: (30759, 3)\n",
      "Number of rows and columns in test data: (10546, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check number of rows and columns\n",
    "print('Number of rows and columns in train data: {}'.format(train_df.shape))\n",
    "print('Number of rows and columns in test data: {}'.format(test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing data in any of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "message      0\n",
       "tweetid      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message    0\n",
       "tweetid    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many posts are there for each sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>2793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           message\n",
       "sentiment         \n",
       " 1           16073\n",
       " 2            6493\n",
       " 0            5400\n",
       "-1            2793"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_sum = train_df.groupby(['sentiment']).count()\n",
    "post_sum.sort_values('message', ascending=False, inplace=True)\n",
    "post_sum = post_sum.drop([\"tweetid\"], axis=1)\n",
    "post_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive sentiments have the most posts combined. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "The sentiment that has the least posts is '-1', it <br> \n",
    "seems that most people believe in climate change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the total words written by each sentiment type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>50561.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word_count\n",
       "sentiment            \n",
       " 1           295413.0\n",
       " 2            99547.0\n",
       " 0            87417.0\n",
       "-1            50561.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['word_count'] = train_df['message'].apply(lambda x: len(str(x).split(\" \")))\n",
    "word_count = train_df.groupby('sentiment').sum()\n",
    "word_count = word_count.drop([\"tweetid\"], axis=1)\n",
    "word_count.sort_values('word_count', ascending=False, inplace=True)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the most words are written by the posts <br>\n",
    "with a positive sentiment. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "The sentiment that had the least words was '-1', <br> \n",
    "and follows the trend above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the Sentiment Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup chart size\n",
    "dim = (15.0, 4.0)\n",
    "fig, ax = plt.subplots(figsize=dim)\n",
    "\n",
    "\n",
    "# Create color palette\n",
    "cmrmap = sns.color_palette('YlGn')\n",
    "sns.set_palette(cmrmap)\n",
    "\n",
    "# Connect data to chart\n",
    "sns.countplot(x='sentiment', data=train_df, order=[-1, 0, 1, 2])\n",
    "\n",
    "# Create labels\n",
    "plt.title('Distribution of Sentiments in the Dataset', fontsize=16)\n",
    "plt.xlabel('Sentiment Value')\n",
    "plt.ylabel('Count of Posts')\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Anti\" sentiment has the least posts, <br>\n",
    "whilst the \"Pro\" sentiment has the most posts. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "This indicates that the data is imbalanced, <br>\n",
    "and we will need to apply sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for wordclouds goes between here and Sentiment Analysis section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made use of Vader to do sentiment analysis on each tweet.<br>\n",
    "VADER belongs to a type of sentiment analysis that is based on lexicons of sentiment-related words.<br>\n",
    "In this approach, each of the words in the lexicon is rated as to whether it is positive or negative, and in many cases, how positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate SentimentIntensityAnalyzer object\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Creates a new column called scores\n",
    "train_df['scores'] = train_df['message'].apply(lambda message: sid.polarity_scores(message))\n",
    "test_df['scores'] = test_df['message'].apply(lambda message: sid.polarity_scores(message))\n",
    "\n",
    "\n",
    "# Creates a new column called compound that contains the compound score of each tweet\n",
    "train_df['compound']  = train_df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "test_df['compound']  = test_df['scores'].apply(lambda score_dict: score_dict['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>word_count</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @darreljorstad: Funny as hell! Canada deman...</td>\n",
       "      <td>897853122080407553</td>\n",
       "      <td>21</td>\n",
       "      <td>{'neg': 0.19, 'neu': 0.698, 'pos': 0.112, 'com...</td>\n",
       "      <td>-0.4574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>All the biggest lies about climate change and ...</td>\n",
       "      <td>925046776553529344</td>\n",
       "      <td>16</td>\n",
       "      <td>{'neg': 0.152, 'neu': 0.761, 'pos': 0.087, 'co...</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>The Coming Revelation Of The $q$Global Warming...</td>\n",
       "      <td>696354236850786305</td>\n",
       "      <td>18</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @DineshDSouza: Let's see if the world ends ...</td>\n",
       "      <td>846806509732483072</td>\n",
       "      <td>21</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @SteveSGoddard: Obama has no control over t...</td>\n",
       "      <td>628085266293653504</td>\n",
       "      <td>19</td>\n",
       "      <td>{'neg': 0.27, 'neu': 0.73, 'pos': 0.0, 'compou...</td>\n",
       "      <td>-0.7430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  RT @darreljorstad: Funny as hell! Canada deman...   \n",
       "1         -1  All the biggest lies about climate change and ...   \n",
       "2         -1  The Coming Revelation Of The $q$Global Warming...   \n",
       "3         -1  RT @DineshDSouza: Let's see if the world ends ...   \n",
       "4         -1  RT @SteveSGoddard: Obama has no control over t...   \n",
       "\n",
       "              tweetid  word_count  \\\n",
       "0  897853122080407553          21   \n",
       "1  925046776553529344          16   \n",
       "2  696354236850786305          18   \n",
       "3  846806509732483072          21   \n",
       "4  628085266293653504          19   \n",
       "\n",
       "                                              scores  compound  \n",
       "0  {'neg': 0.19, 'neu': 0.698, 'pos': 0.112, 'com...   -0.4574  \n",
       "1  {'neg': 0.152, 'neu': 0.761, 'pos': 0.087, 'co...   -0.2960  \n",
       "2  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000  \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000  \n",
       "4  {'neg': 0.27, 'neu': 0.73, 'pos': 0.0, 'compou...   -0.7430  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'comp...</td>\n",
       "      <td>0.6310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>{'neg': 0.167, 'neu': 0.833, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "      <td>{'neg': 0.198, 'neu': 0.802, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "      <td>{'neg': 0.107, 'neu': 0.893, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "      <td>{'neg': 0.223, 'neu': 0.777, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  Europe will now be looking to China to make su...   169760   \n",
       "1  Combine this with the polling of staffers re c...    35326   \n",
       "2  The scary, unimpeachable evidence that climate...   224985   \n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263   \n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928   \n",
       "\n",
       "                                              scores  compound  \n",
       "0  {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'comp...    0.6310  \n",
       "1  {'neg': 0.167, 'neu': 0.833, 'pos': 0.0, 'comp...   -0.5574  \n",
       "2  {'neg': 0.198, 'neu': 0.802, 'pos': 0.0, 'comp...   -0.4939  \n",
       "3  {'neg': 0.107, 'neu': 0.893, 'pos': 0.0, 'comp...   -0.3382  \n",
       "4  {'neg': 0.223, 'neu': 0.777, 'pos': 0.0, 'comp...   -0.3164  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now assign the sentiment 'Postive', 'Negative or 'Neutral' based on the compound score of the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of compound scores in train dataset\n",
    "compound = list(train_df['compound'])\n",
    "\n",
    "# instantiate empty list\n",
    "com =[]\n",
    "\n",
    "# assign 'Postive', 'Negative or 'Neutral' based on the compound score of the tweet.\n",
    "for i in compound:\n",
    "    if i >= 0.05:\n",
    "        com.append(' .Positive.')\n",
    "    elif i > -0.05 and i <0.05:\n",
    "        com.append(' .Neutral.')\n",
    "    elif i <= -0.05 : #or else\n",
    "        com.append(' .Negative.')\n",
    "\n",
    "# update training set dataframe\n",
    "train_df['calculated_sent'] = com\n",
    "\n",
    "# create list of compound scores in test set dataframe\n",
    "compound = list(test_df['compound'])\n",
    "\n",
    "# instantiate empty list\n",
    "com =[]\n",
    "\n",
    "# assign 'Postive', 'Negative or 'Neutral' based on the compound score of the tweet.\n",
    "for i in compound:\n",
    "    if i >= 0.05:\n",
    "        com.append(' .Positive.')\n",
    "    elif i > -0.05 and i <0.05:\n",
    "        com.append(' .Neutral.')\n",
    "    elif i <= -0.05 : #or else\n",
    "        com.append(' .Negative.')\n",
    "        \n",
    "# update training set dataframe\n",
    "test_df['calculated_sent'] = com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>word_count</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>calculated_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @darreljorstad: Funny as hell! Canada deman...</td>\n",
       "      <td>897853122080407553</td>\n",
       "      <td>21</td>\n",
       "      <td>{'neg': 0.19, 'neu': 0.698, 'pos': 0.112, 'com...</td>\n",
       "      <td>-0.4574</td>\n",
       "      <td>.Negative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>All the biggest lies about climate change and ...</td>\n",
       "      <td>925046776553529344</td>\n",
       "      <td>16</td>\n",
       "      <td>{'neg': 0.152, 'neu': 0.761, 'pos': 0.087, 'co...</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>.Negative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>The Coming Revelation Of The $q$Global Warming...</td>\n",
       "      <td>696354236850786305</td>\n",
       "      <td>18</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>.Neutral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @DineshDSouza: Let's see if the world ends ...</td>\n",
       "      <td>846806509732483072</td>\n",
       "      <td>21</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>.Neutral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>RT @SteveSGoddard: Obama has no control over t...</td>\n",
       "      <td>628085266293653504</td>\n",
       "      <td>19</td>\n",
       "      <td>{'neg': 0.27, 'neu': 0.73, 'pos': 0.0, 'compou...</td>\n",
       "      <td>-0.7430</td>\n",
       "      <td>.Negative.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1  RT @darreljorstad: Funny as hell! Canada deman...   \n",
       "1         -1  All the biggest lies about climate change and ...   \n",
       "2         -1  The Coming Revelation Of The $q$Global Warming...   \n",
       "3         -1  RT @DineshDSouza: Let's see if the world ends ...   \n",
       "4         -1  RT @SteveSGoddard: Obama has no control over t...   \n",
       "\n",
       "              tweetid  word_count  \\\n",
       "0  897853122080407553          21   \n",
       "1  925046776553529344          16   \n",
       "2  696354236850786305          18   \n",
       "3  846806509732483072          21   \n",
       "4  628085266293653504          19   \n",
       "\n",
       "                                              scores  compound calculated_sent  \n",
       "0  {'neg': 0.19, 'neu': 0.698, 'pos': 0.112, 'com...   -0.4574      .Negative.  \n",
       "1  {'neg': 0.152, 'neu': 0.761, 'pos': 0.087, 'co...   -0.2960      .Negative.  \n",
       "2  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000       .Neutral.  \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000       .Neutral.  \n",
       "4  {'neg': 0.27, 'neu': 0.73, 'pos': 0.0, 'compou...   -0.7430      .Negative.  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>calculated_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'comp...</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>.Positive.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "      <td>{'neg': 0.167, 'neu': 0.833, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>.Negative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "      <td>{'neg': 0.198, 'neu': 0.802, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>.Negative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "      <td>{'neg': 0.107, 'neu': 0.893, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3382</td>\n",
       "      <td>.Negative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "      <td>{'neg': 0.223, 'neu': 0.777, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.3164</td>\n",
       "      <td>.Negative.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid  \\\n",
       "0  Europe will now be looking to China to make su...   169760   \n",
       "1  Combine this with the polling of staffers re c...    35326   \n",
       "2  The scary, unimpeachable evidence that climate...   224985   \n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263   \n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928   \n",
       "\n",
       "                                              scores  compound calculated_sent  \n",
       "0  {'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'comp...    0.6310      .Positive.  \n",
       "1  {'neg': 0.167, 'neu': 0.833, 'pos': 0.0, 'comp...   -0.5574      .Negative.  \n",
       "2  {'neg': 0.198, 'neu': 0.802, 'pos': 0.0, 'comp...   -0.4939      .Negative.  \n",
       "3  {'neg': 0.107, 'neu': 0.893, 'pos': 0.0, 'comp...   -0.3382      .Negative.  \n",
       "4  {'neg': 0.223, 'neu': 0.777, 'pos': 0.0, 'comp...   -0.3164      .Negative.  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the sentiment to the end of the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['message'] = train_df['message'].astype(str).add(train_df['calculated_sent'])\n",
    "test_df['message'] = test_df['message'].astype(str).add(test_df['calculated_sent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/data-preprocessing.png\" width = \"60%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cleaning of our data we will be making use of multiple functions. Some of which we found on the internet and other kaggle notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_lower(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a string of text as input. It makes the strings lowercase and returns the modified text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # return lowercase\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def clean_url(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a string of text as input. It replaces the url with with the string 'web-url\n",
    "    from the text string and returns the modified text string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # define RegEx pattern for a url link\n",
    "    pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "    \n",
    "    # define replacement word\n",
    "    subs_url = r'web-url'\n",
    "    \n",
    "    # replace urls with 'web-url'\n",
    "    return re.sub(pattern_url, subs_url, text)\n",
    "    \n",
    "    \n",
    "def remove_punctuation_numbers(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a string of text as input. It removes punctuation and numbers from the text string\n",
    "    and returns the modified text string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # define punctuation and numbers\n",
    "    punc_numbers = string.punctuation + '0123456789'\n",
    "    \n",
    "    \n",
    "    # return the modeified text\n",
    "    return ' '.join([l for l in text.split() if l not in punc_numbers])\n",
    "\n",
    "\n",
    "def clean(tweet):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes some words that are contracted and expands them\n",
    "    \"\"\"\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes a string of text as input. It corrects the spelling by applying textblob's correction\n",
    "    method and returns the modified text string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # instantiate TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # correct spelling and return modified string\n",
    "    return str(blob.correct())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning URLs...\n",
      "Removing punctuation...\n",
      "Lowering case...\n",
      "Removing rt...\n",
      "Expanding words...\n"
     ]
    }
   ],
   "source": [
    "## Clean urls\n",
    "print ('Cleaning URLs...')\n",
    "\n",
    "train_df['message'] = train_df['message'].apply(clean_url)\n",
    "test_df['message'] = test_df['message'].apply(clean_url)\n",
    "\n",
    "# Remove punctuation\n",
    "print ('Removing punctuation...')\n",
    "train_df['message'] = train_df['message'].apply(remove_punctuation_numbers)\n",
    "test_df['message'] = test_df['message'].apply(remove_punctuation_numbers)\n",
    "\n",
    "\n",
    "# Make lower case\n",
    "print ('Lowering case...')\n",
    "\n",
    "train_df['message'] = train_df['message'].apply(make_lower)\n",
    "test_df['message'] = test_df['message'].apply(make_lower)\n",
    "\n",
    "\n",
    "## Remove rt\n",
    "print ('Removing rt...')\n",
    "\n",
    "train_df['message'] = train_df['message'].replace(to_replace = r'rt', value = '', regex = True)\n",
    "test_df['message'] = test_df['message'].replace(to_replace = r'rt', value = '', regex = True)\n",
    "\n",
    "##cleaning function\n",
    "print('Expanding words...')\n",
    "train_df['message'] = train_df['message'].apply(clean)\n",
    "test_df['message'] = test_df['message'].apply(clean)\n",
    "\n",
    "# correct spelling\n",
    "#print ('Removing punctuation...')\n",
    "#train_df['message'] = train_df['message'].apply(correct_spelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are Text Normalization (or sometimes called Word Normalization) techniques in the field of Natural Language Processing that are used to prepare text, words, and documents for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing both stemming and lemmatization on our text we discovered that using lemmatization has better results \n",
    "than when stemming is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying lemmatization to the train and test message data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train_df['lemma'] = [' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])for text in train_df['message']]\n",
    "test_df['lemma'] = [' '.join([lemmatizer.lemmatize(word) for word in text.split(' ')])for text in test_df['message']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0Y2Y6gLtT1d"
   },
   "source": [
    "## Word Vectors (Feature Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Count Vectorizer as well as  Tfidf Vectorizer. <br>\n",
    "We decided to use the full set of features(tfidf+counts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['lemma']\n",
    "y = train_df['sentiment']\n",
    "X_test = test_df['lemma']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(min_df=1, max_df=0.9, stop_words='english', decode_error='ignore')\n",
    "vectorized=vectorizer.fit_transform(X)\n",
    "\n",
    "count_vectorizer=CountVectorizer(min_df=1, max_df=0.9, stop_words='english', decode_error='ignore')\n",
    "count_vectorized=count_vectorizer.fit_transform(X)\n",
    "X = scipy.sparse.hstack([vectorized, count_vectorized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer=TfidfVectorizer(min_df=1, max_df=0.9, stop_words='english', decode_error='ignore')\n",
    "vectorized=vectorizer.transform(X_test)\n",
    "\n",
    "#count_vectorizer=CountVectorizer(min_df=1, max_df=0.9, stop_words='english', decode_error='ignore')\n",
    "count_vectorized=count_vectorizer.transform(X_test)\n",
    "\n",
    "X_test = scipy.sparse.hstack([vectorized, count_vectorized])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting our train data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size =0.2,\n",
    "                                                   random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seC8yfNgtT1g"
   },
   "source": [
    "## Classification Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8JSevzftT1h"
   },
   "source": [
    "1. Connect Model Experiment to comet\n",
    "2. Train Model\n",
    "3. Test and Evaluate Model\n",
    "4. Deploy Model\n",
    "\n",
    "    4.1 Deploy in Kaggle\n",
    "    \n",
    "    4.2 Deploy in Comet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Functions for Model Evaluation and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "def deploy_comet(experiment, metrics, parameters=None):\n",
    "    \"\"\"\n",
    "    This function takes a comet experiment object, a dictionary of model parameters\n",
    "    and a dictionary of model test results as inputs and uploads the experiment to comet.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log our parameters\n",
    "    if parameters != None:\n",
    "        print('logging parameters...')\n",
    "        experiment.log_parameters(params)\n",
    "    \n",
    "    # log model performace\n",
    "    print('logging metric...')\n",
    "    experiment.log_metrics(metrics)\n",
    "    \n",
    "    print('ending experiment...')\n",
    "    # end experiment\n",
    "    experiment.end()  \n",
    "\n",
    "    \n",
    "    \n",
    "def dict_to_df(dict_data):\n",
    "    \"\"\"\n",
    "    This function take a dictionary of data and returns a dataframe of the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert dictionary to dataframe\n",
    "    return pd.DataFrame(dict_data, index=['Value']).T\n",
    "\n",
    "\n",
    "\n",
    "def make_submission_csv(model_name, clf, features):\n",
    "    \n",
    "    \"\"\"\n",
    "    This funtion this model take a file name as string, model object and vector of features\n",
    "    as inputs and returns a csv to submit to Kaggle or Zindi\n",
    "    \"\"\"\n",
    "    \n",
    "    # define columns in sample submission data\n",
    "    cols = sample_submission_df.columns\n",
    "    \n",
    "    # Make prediction of submission data\n",
    "    print('making prediction...')\n",
    "    prediction = clf.predict(features)\n",
    "    \n",
    "    # make dataframe of predictions of submission data\n",
    "    print('creating dataframe... ')\n",
    "    submission = pd.DataFrame({cols[0]: sample_submission_df[cols[0]],\n",
    "                               cols[1]: prediction})\n",
    "    \n",
    "    # save file\n",
    "    filename = 'submission_' + model_name + '.csv'\n",
    "    submission.to_csv(filename,index=False)\n",
    "    print(f'Saved file {filename}')\n",
    "\n",
    "\n",
    "def make_pickle(model_name, model):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes the string of a model name and model object as input and\n",
    "    saves a pickle file of the model in the current directory of the file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create file name\n",
    "    model_save_path = model_name + \".pkl\"\n",
    "\n",
    "    # open file to write binary\n",
    "    with open(model_save_path,'wb') as file:\n",
    "\n",
    "        # save model as file name\n",
    "        pickle.dump(model,file)\n",
    "        print(f'Pickle file for {model_name} successfully saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XApup1ydtT1j"
   },
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/model-building.png\" width = \"50%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1c-8UHMtT1j"
   },
   "source": [
    "#### Base 1 Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =   LinearSVC().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 145   35   83   15]\n",
      " [  20  198  174   33]\n",
      " [  42  130 1420  163]\n",
      " [   6   18  131  551]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.68      0.52      0.59       278\n",
      "           0       0.52      0.47      0.49       425\n",
      "           1       0.79      0.81      0.80      1755\n",
      "           2       0.72      0.78      0.75       706\n",
      "\n",
      "    accuracy                           0.73      3164\n",
      "   macro avg       0.68      0.64      0.66      3164\n",
      "weighted avg       0.73      0.73      0.73      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.731353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.726610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.731353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.727517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.731353\n",
       "precision  0.726610\n",
       "recall     0.731353\n",
       "f1_score   0.727517"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Model Performace\n",
    "accuracy_base_lsvc = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_base_lsvc = metrics.precision_score(y_val,y_pred, average ='weighted')\n",
    "recall_base_lsvc = metrics.recall_score(y_val,y_pred, average ='weighted')\n",
    "f1_base_lsvc = metrics.f1_score(y_val,y_pred, average ='weighted')\n",
    "\n",
    "# save model metrics\n",
    "metrics_base_lsvc = {'accuracy': accuracy_base_lsvc,\n",
    "                     'precision': precision_base_lsvc,\n",
    "                     'recall': recall_base_lsvc,\n",
    "                     'f1_score': f1_base_lsvc}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_base_lsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_base_lsvc.csv\n",
      "Pickle file for base_lsvc successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('base_lsvc', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('base_lsvc', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQtpkkujtT1l"
   },
   "source": [
    "#### Base 2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  MultinomialNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  51   20  179   28]\n",
      " [   8  116  255   46]\n",
      " [   2   28 1553  172]\n",
      " [   1    4  171  530]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.18      0.30       278\n",
      "           0       0.69      0.27      0.39       425\n",
      "           1       0.72      0.88      0.79      1755\n",
      "           2       0.68      0.75      0.72       706\n",
      "\n",
      "    accuracy                           0.71      3164\n",
      "   macro avg       0.73      0.52      0.55      3164\n",
      "weighted avg       0.72      0.71      0.68      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.711125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.716594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.711125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.678791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.711125\n",
       "precision  0.716594\n",
       "recall     0.711125\n",
       "f1_score   0.678791"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Model Performace\n",
    "accuracy_nb = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_nb = metrics.precision_score(y_val,y_pred, average = 'weighted')\n",
    "recall_nb = metrics.recall_score(y_val,y_pred, average = 'weighted')\n",
    "f1_nb = metrics.f1_score(y_val,y_pred, average = 'weighted')\n",
    "\n",
    "# save model metrics\n",
    "metrics_base_nb = {'accuracy': accuracy_nb,\n",
    "                     'precision': precision_nb,\n",
    "                     'recall': recall_nb,\n",
    "                     'f1_score': f1_nb}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_base_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_base_mnb.csv\n",
      "Pickle file for base_mnb successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('base_mnb', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('base_mnb', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyD_yDOUtT1m"
   },
   "source": [
    "#### Base 3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf =  LogisticRegression().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 131   36  100   11]\n",
      " [  17  198  186   24]\n",
      " [  26  100 1502  127]\n",
      " [   5   19  130  552]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.47      0.57       278\n",
      "           0       0.56      0.47      0.51       425\n",
      "           1       0.78      0.86      0.82      1755\n",
      "           2       0.77      0.78      0.78       706\n",
      "\n",
      "    accuracy                           0.75      3164\n",
      "   macro avg       0.71      0.64      0.67      3164\n",
      "weighted avg       0.75      0.75      0.75      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.753161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.746525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.753161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.745871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.753161\n",
       "precision  0.746525\n",
       "recall     0.753161\n",
       "f1_score   0.745871"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Model Performace\n",
    "accuracy_log = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_log = metrics.precision_score(y_val,y_pred, average = 'weighted')\n",
    "recall_log = metrics.recall_score(y_val,y_pred, average = 'weighted')\n",
    "f1_log = metrics.f1_score(y_val,y_pred, average = 'weighted')\n",
    "\n",
    "# save model metrics\n",
    "metrics_base_lr = {'accuracy': accuracy_log,\n",
    "                     'precision': precision_log,\n",
    "                     'recall': recall_log,\n",
    "                     'f1_score': f1_log}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_base_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_base_lr.csv\n",
      "Pickle file for base_lr successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('base_lr', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('base_lr', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lBcD3pvtT1n"
   },
   "source": [
    "#### Base 4 K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  49  133   94    2]\n",
      " [  16  309   98    2]\n",
      " [  39  653 1006   57]\n",
      " [   3  201  248  254]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.46      0.18      0.25       278\n",
      "           0       0.24      0.73      0.36       425\n",
      "           1       0.70      0.57      0.63      1755\n",
      "           2       0.81      0.36      0.50       706\n",
      "\n",
      "    accuracy                           0.51      3164\n",
      "   macro avg       0.55      0.46      0.43      3164\n",
      "weighted avg       0.64      0.51      0.53      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.511378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.638084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.511378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.530266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.511378\n",
       "precision  0.638084\n",
       "recall     0.511378\n",
       "f1_score   0.530266"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_kn = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_kn = metrics.precision_score(y_val,y_pred, average = 'weighted')\n",
    "recall_kn = metrics.recall_score(y_val,y_pred, average = 'weighted')\n",
    "f1_kn = metrics.f1_score(y_val,y_pred, average = 'weighted')\n",
    "\n",
    "# save model metrics\n",
    "metrics_base_knn = {'accuracy': accuracy_kn,\n",
    "                     'precision': precision_kn,\n",
    "                     'recall': recall_kn,\n",
    "                     'f1_score': f1_kn}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_base_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_base_knn.csv\n",
      "Pickle file for base_knn successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('base_knn', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('base_knn', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mytlnzOBtT1q"
   },
   "source": [
    "#### Base 6 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(class_weight = \"balanced\").fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  62   32  171   13]\n",
      " [   3  175  214   33]\n",
      " [   4   97 1471  183]\n",
      " [   0   13  182  511]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.90      0.22      0.36       278\n",
      "           0       0.55      0.41      0.47       425\n",
      "           1       0.72      0.84      0.78      1755\n",
      "           2       0.69      0.72      0.71       706\n",
      "\n",
      "    accuracy                           0.70      3164\n",
      "   macro avg       0.72      0.55      0.58      3164\n",
      "weighted avg       0.71      0.70      0.68      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.701327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.707546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.701327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.682695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.701327\n",
       "precision  0.707546\n",
       "recall     0.701327\n",
       "f1_score   0.682695"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_rf = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_rf = metrics.precision_score(y_val,y_pred, average = 'weighted')\n",
    "recall_rf = metrics.recall_score(y_val,y_pred, average = 'weighted')\n",
    "f1_rf = metrics.f1_score(y_val,y_pred, average = 'weighted')\n",
    "\n",
    "# save model metrics\n",
    "metrics_base_rf = {'accuracy': accuracy_rf,\n",
    "                     'precision': precision_rf,\n",
    "                     'recall': recall_rf,\n",
    "                     'f1_score': f1_rf}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_base_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_base_rf.csv\n",
      "Pickle file for base_rf successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('base_rf', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('base_rf', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YezMZrhwfyKU"
   },
   "source": [
    "#### Base 7 Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [accuracy_log,accuracy_sv, accuracy_nb, accuracy_kn, accuracy_rf]\n",
    "f1_score = [f1_log, f1_sv, f1_nb, f1_kn, f1_rf]\n",
    "results_base = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "results_base['F1_base'] = f1_score\n",
    "results_base['Accuracy_base'] = accuracy\n",
    "results_base['models'] = ['Logistic Regression', 'Linear Support Vector', 'Naive Bayes', 'K Neighbours', 'Random Forest']\n",
    "results_base.set_index('models', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HyPYl95tT1s"
   },
   "source": [
    "### Tuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFa2LW06tT1u"
   },
   "source": [
    "#### Tuned 1 Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_LSVC_model(X_train, y_train): \n",
    "    C_list = [0.001, 0.01, 0.1, 0.5, 0.75, 1, 5, 10, 25, 100]\n",
    "    penalty_list = ['l1','l2']\n",
    "    loss =['hinge', 'squared_hinge']\n",
    "   \n",
    "    scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n",
    "\n",
    "\n",
    "    \n",
    "    lsvc = LinearSVC()\n",
    "    \n",
    "    parameters = {'C':C_list,\n",
    "                  'penalty': penalty_list,\n",
    "                  'loss' : loss\n",
    "                     }\n",
    "    tune = GridSearchCV(lsvc, parameters, scoring = scorer)\n",
    "    tune.fit(X_train,y_train)\n",
    "    \n",
    "    return tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tune_LSVC_model(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 128   38   99   13]\n",
      " [  13  194  185   33]\n",
      " [  19   92 1500  144]\n",
      " [   6    9  132  559]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(metrics.confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.46      0.58       278\n",
      "           0       0.58      0.46      0.51       425\n",
      "           1       0.78      0.85      0.82      1755\n",
      "           2       0.75      0.79      0.77       706\n",
      "\n",
      "    accuracy                           0.75      3164\n",
      "   macro avg       0.72      0.64      0.67      3164\n",
      "weighted avg       0.75      0.75      0.74      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.752528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.746784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.752528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.744162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.752528\n",
       "precision  0.746784\n",
       "recall     0.752528\n",
       "f1_score   0.744162"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model performanace\n",
    "accuracy_sv = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_sv = metrics.precision_score(y_val,y_pred, average ='weighted')\n",
    "recall_sv = metrics.recall_score(y_val,y_pred, average ='weighted')\n",
    "f1_sv = metrics.f1_score(y_val,y_pred, average ='weighted')\n",
    "\n",
    "# save performance metrics\n",
    "metrics_tuned_lsvc = {'accuracy': accuracy_sv,\n",
    "                     'precision': precision_sv,\n",
    "                     'recall': recall_sv,\n",
    "                     'f1_score': f1_sv}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_tuned_lsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_tuned_lsvc.csv\n",
      "Pickle file for tuned_lsvc successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('tuned_lsvc', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('tuned_lsvc', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnFyeMcytT1v"
   },
   "source": [
    "#### Adjusted 2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_NB_model(X_train, y_train): \n",
    "    alpha = ['alpha', 1e-10, 1]\n",
    "    fit_prior = ['fit_prior', [True, False]]\n",
    "    scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n",
    "\n",
    "\n",
    "    \n",
    "    nb = MultinomialNB()\n",
    "    \n",
    "    parameters = {'alpha': alpha,\n",
    "                  'fit_prior': fit_prior\n",
    "                     }\n",
    "    tune = GridSearchCV(nb, parameters, scoring = scorer)\n",
    "    tune.fit(X_train,y_train)\n",
    "    \n",
    "    return tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tune_NB_model(X_train, y_train)\n",
    "#y_pred = tune_nb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 137   35   91   15]\n",
      " [  63  168  141   53]\n",
      " [  96  176 1261  222]\n",
      " [  11   30  159  506]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(metrics.confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.45      0.49      0.47       278\n",
      "           0       0.41      0.40      0.40       425\n",
      "           1       0.76      0.72      0.74      1755\n",
      "           2       0.64      0.72      0.67       706\n",
      "\n",
      "    accuracy                           0.65      3164\n",
      "   macro avg       0.56      0.58      0.57      3164\n",
      "weighted avg       0.66      0.65      0.66      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.654867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.659621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.654867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.656206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.654867\n",
       "precision  0.659621\n",
       "recall     0.654867\n",
       "f1_score   0.656206"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Model Performance\n",
    "accuracy_nb = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_nb = metrics.precision_score(y_val,y_pred, average = 'weighted')\n",
    "recall_nb = metrics.recall_score(y_val,y_pred, average = 'weighted')\n",
    "f1_nb = metrics.f1_score(y_val,y_pred, average = 'weighted')\n",
    "\n",
    "\n",
    "# save performance metrics\n",
    "metrics_tuned_nb = {'accuracy': accuracy_nb,\n",
    "                     'precision': precision_nb,\n",
    "                     'recall': recall_nb,\n",
    "                     'f1_score': f1_nb}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_tuned_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_tuned_naive_beyes.csv\n",
      "Pickle file for tuned_naive_beyes successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('tuned_naive_beyes', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('tuned_naive_beyes', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRA9FLqHtT1w"
   },
   "source": [
    " #### Adjusted 3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_LogReg_model(X_train, y_train): \n",
    "    C_list = [0.001, 0.01, 0.1, 0.5, 0.75, 1, 5, 10, 25, 100]\n",
    "    penalty_list = ['l1','l2']\n",
    "    random_state = ['random_state', 1, 10]\n",
    "    tol = ['tol', 1e-10, 1]\n",
    "    max_iter = ['max_iter', 1000, 10000]\n",
    "    warm_start = ['warm_start', [True, False]]\n",
    "   \n",
    "    scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n",
    "\n",
    "\n",
    "    \n",
    "    logreg = LogisticRegression()\n",
    "    \n",
    "    parameters = {'C':C_list,\n",
    "                  'penalty': penalty_list,\n",
    "                  'random_state' : random_state,\n",
    "                  'tol': tol,\n",
    "                  \n",
    "                     }\n",
    "    tune = GridSearchCV(logreg, parameters, scoring = scorer)\n",
    "    tune.fit(X_train,y_train)\n",
    "    \n",
    "    return tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-791db25599c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_LogReg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": [
    "clf = tune_LogReg_model(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.75, random_state=1, tol=1e-10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 131   36  100   11]\n",
      " [  17  198  186   24]\n",
      " [  26  100 1502  127]\n",
      " [   5   19  130  552]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(metrics.confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.73      0.47      0.57       278\n",
      "           0       0.56      0.47      0.51       425\n",
      "           1       0.78      0.86      0.82      1755\n",
      "           2       0.77      0.78      0.78       706\n",
      "\n",
      "    accuracy                           0.75      3164\n",
      "   macro avg       0.71      0.64      0.67      3164\n",
      "weighted avg       0.75      0.75      0.75      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.753161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.746525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.753161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.745871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.753161\n",
       "precision  0.746525\n",
       "recall     0.753161\n",
       "f1_score   0.745871"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_log = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_log = metrics.precision_score(y_val,y_pred, average = 'weighted')\n",
    "recall_log = metrics.recall_score(y_val,y_pred, average = 'weighted')\n",
    "f1_log = metrics.f1_score(y_val,y_pred, average = 'weighted')\n",
    "\n",
    "# save performance metrics\n",
    "metrics_tuned_lr = {'accuracy': accuracy_log,\n",
    "                     'precision': precision_log,\n",
    "                     'recall': recall_log,\n",
    "                     'f1_score': f1_log}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_tuned_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_tuned_logistic_regression.csv\n",
      "Pickle file for tuned_logistic_regression successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('tuned_logistic_regression', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('tuned_logistic_regression', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ0aW4zmtT1x"
   },
   "source": [
    "#### Adjusted 4 K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_KNN_model(X_train, y_train): \n",
    "    \n",
    "    algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50, 100]\n",
    "        \n",
    "    scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n",
    "\n",
    "\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    \n",
    "    parameters = {'algorithm': algorithm,\n",
    "                  'n_neighbors': ks\n",
    "                     }\n",
    "    tune = GridSearchCV(knn, parameters, scoring = scorer)\n",
    "    tune.fit(X_train,y_train)\n",
    " \n",
    "    \n",
    "    return tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tune_kn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-687857811105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_KNN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_kn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tune_kn' is not defined"
     ]
    }
   ],
   "source": [
    "clf = tune_KNN_model(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 72 110  87   9]\n",
      " [ 45 274 101   5]\n",
      " [118 557 985  95]\n",
      " [ 60 104 182 360]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.24      0.26      0.25       278\n",
      "           0       0.26      0.64      0.37       425\n",
      "           1       0.73      0.56      0.63      1755\n",
      "           2       0.77      0.51      0.61       706\n",
      "\n",
      "    accuracy                           0.53      3164\n",
      "   macro avg       0.50      0.49      0.47      3164\n",
      "weighted avg       0.63      0.53      0.56      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.534450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.631157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.534450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.560240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.534450\n",
       "precision  0.631157\n",
       "recall     0.534450\n",
       "f1_score   0.560240"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_kn = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_kn = metrics.precision_score(y_val,y_pred, average = 'weighted')\n",
    "recall_kn = metrics.recall_score(y_val,y_pred, average = 'weighted')\n",
    "f1_kn = metrics.f1_score(y_val,y_pred, average = 'weighted')\n",
    "\n",
    "# save performance metrics\n",
    "metrics_tuned_knn = {'accuracy': accuracy_kn,\n",
    "                     'precision': precision_kn,\n",
    "                     'recall': recall_kn,\n",
    "                     'f1_score': f1_kn}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_tuned_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_tuned_k_nearest_neighbour.csv\n",
      "Pickle file for tuned_k_nearest_neighbour successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('tuned_k_nearest_neighbour', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('tuned_k_nearest_neighbour', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3t9GbUuGtT10"
   },
   "source": [
    "#### Adjusted 6 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_RF_model(X_train, y_train): \n",
    "    \n",
    "    n_estimators = [10,20,30,50,100]\n",
    "    max_depth = [1,5,10]\n",
    "    max_features = ['auto', 'sqrt']   \n",
    "\n",
    "    scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n",
    "\n",
    "\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    \n",
    "    parameters = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               \n",
    "                 }\n",
    "    \n",
    "    tune = GridSearchCV(rf, parameters, scoring = scorer)\n",
    "    tune.fit(X_train,y_train)\n",
    " \n",
    "    \n",
    "    return tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tune_RF_model(X_train,y_train)\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resources/models/Logistic_regression.pkl'\n",
    "\n",
    "model_save_path = 'resources/models/Logistic_regression.pkl'\n",
    "\n",
    "# open file to write binary\n",
    "with open(model_save_path,'wb') as file:\n",
    "\n",
    "    # save model as file name\n",
    "    pickle.dump(clf.best_estimator_,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.23      0.37       278\n",
      "           0       0.61      0.32      0.42       425\n",
      "           1       0.70      0.89      0.78      1755\n",
      "           2       0.75      0.68      0.71       706\n",
      "\n",
      "    accuracy                           0.71      3164\n",
      "   macro avg       0.75      0.53      0.57      3164\n",
      "weighted avg       0.72      0.71      0.68      3164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_val,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.709545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.719845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.709545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.683602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Value\n",
       "accuracy   0.709545\n",
       "precision  0.719845\n",
       "recall     0.709545\n",
       "f1_score   0.683602"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Model Performance\n",
    "accuracy_rf = metrics.accuracy_score(y_val,y_pred)\n",
    "precision_rf = metrics.precision_score(y_val,y_pred, average = 'weighted')\n",
    "recall_rf = metrics.recall_score(y_val,y_pred, average = 'weighted')\n",
    "f1_rf = metrics.f1_score(y_val,y_pred, average = 'weighted')\n",
    "\n",
    "# save performance metrics\n",
    "metrics_tuned_rf = {'accuracy': accuracy_rf,\n",
    "                     'precision': precision_rf,\n",
    "                     'recall': recall_rf,\n",
    "                     'f1_score': f1_rf}\n",
    "\n",
    "# show model performance\n",
    "dict_to_df(metrics_tuned_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making prediction...\n",
      "creating dataframe... \n",
      "Saved file submission_tuned_random_forest.csv\n",
      "Pickle file for tuned_random_forest successfully saved\n"
     ]
    }
   ],
   "source": [
    "# deploy model on comet\n",
    "# deploy_comet(experiment, metrics_base_lsvc, lr.get_params())\n",
    "\n",
    "# save to Kaggle\n",
    "make_submission_csv('tuned_random_forest', clf, X_test)\n",
    "\n",
    "# save pickle file\n",
    "make_pickle('tuned_random_forest', clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/model-prediction.png\" width = \"50%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [accuracy_log,accuracy_sv, accuracy_nb, accuracy_kn, accuracy_rf]\n",
    "f1 = [f1_log, f1_sv, f1_nb, f1_kn, f1_rf]\n",
    "results = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "results['F1'] = f1\n",
    "results['Accuracy'] = acc\n",
    "results['models'] = ['Logistic Regression', 'Linear Support Vector', 'Naive Bayes', 'K Neighbours', 'Random Forest']\n",
    "results.set_index('models', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = pd.DataFrame()\n",
    "\n",
    "results_all['F1_base'] = f1_score\n",
    "results_all['f1_tunned'] =f1\n",
    "results_all['Accuracy_base'] = accuracy\n",
    "results_all['Accuracy_tunned'] = acc\n",
    "\n",
    "results_all['models'] = ['Logistic Regression', 'Linear Support Vector', 'Naive Bayes', 'K Neighbours', 'Random Forest']\n",
    "results_all.set_index('models', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building All Models (Stacked Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Logistic Regression', 'Nearest Neighbors',' Naive Bayes',\n",
    "         'Linear SVM',\n",
    "          'Random Forest',  'AdaBoost']\n",
    "\n",
    "classifiers = [\n",
    "    MultinomialNB(alpha = 1),\n",
    "    KNeighborsClassifier(n_neighbors =1),\n",
    "    LinearSVC(C= 0.1, loss = 'squared_hinge'),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(C=1 , penalty ='l2', random_state =1),\n",
    "    AdaBoostClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "models = {}\n",
    "confusion = {}\n",
    "class_report = {}\n",
    "\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print ('Fitting {:s} model...'.format(name))\n",
    "    run_time = %timeit -q -o clf.fit(X_train, y_train)\n",
    "\n",
    "    print ('... predicting')\n",
    "    y_pred = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_val)\n",
    "\n",
    "    print ('... scoring')\n",
    "    accuracy  = metrics.accuracy_score(y_train, y_pred)\n",
    "    precision = metrics.precision_score(y_train, y_pred, average = 'weighted')\n",
    "    recall    = metrics.recall_score(y_train, y_pred, average = 'weighted')\n",
    "\n",
    "    f1        = metrics.f1_score(y_train, y_pred,average = 'weighted')\n",
    "    f1_test   = metrics.f1_score(y_val, y_pred_test, average = 'weighted')\n",
    "\n",
    "    # Save the results to dictionaries\n",
    "    models[name] = clf\n",
    "    confusion[name] = metrics.confusion_matrix(y_train, y_pred)\n",
    "    class_report[name] = metrics.classification_report(y_train, y_pred)\n",
    "\n",
    "    results.append([name, accuracy, precision, recall, f1, f1_test, run_time.best])\n",
    "\n",
    "\n",
    "results = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Train', 'F1 Test', 'Train Time'])\n",
    "results.set_index('Classifier', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('F1 Test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSmbSwUOtT13"
   },
   "source": [
    "## Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7WQVTGBtT14"
   },
   "source": [
    "Explain How experiment results will be presented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkzvW395tT15"
   },
   "source": [
    "# Insights and Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pd194J7tT16"
   },
   "source": [
    "<img src = \"https://raw.githubusercontent.com/Mikentosh/ss5-classification/master/images/conclusion.png\" width = \"40%\" align = \"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkA51AYrtT17"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
