{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import nltk as nlp\n",
    "import string\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>sentiment_compound</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>@darreljorstad: funny  hell! canada demands '...</td>\n",
       "      <td>897853122080407553</td>\n",
       "      <td>-0.4574</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>biggest lies  climate change  global warming...</td>\n",
       "      <td>925046776553529344</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>coming revelation   $q$global warming$q$ #fra...</td>\n",
       "      <td>696354236850786305</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.071429</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>@dineshdsouza: let's see   world ends  @reald...</td>\n",
       "      <td>846806509732483072</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.120000</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>@stevesgoddard: obama   control   climate.   ...</td>\n",
       "      <td>628085266293653504</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>5.437500</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  \\\n",
       "0         -1   @darreljorstad: funny  hell! canada demands '...   \n",
       "1         -1    biggest lies  climate change  global warming...   \n",
       "2         -1   coming revelation   $q$global warming$q$ #fra...   \n",
       "3         -1   @dineshdsouza: let's see   world ends  @reald...   \n",
       "4         -1   @stevesgoddard: obama   control   climate.   ...   \n",
       "\n",
       "              tweetid  sentiment_compound  avg_word_len  length  \n",
       "0  897853122080407553             -0.4574      5.000000     120  \n",
       "1  925046776553529344             -0.2960      7.000000     106  \n",
       "2  696354236850786305              0.0000      4.071429     131  \n",
       "3  846806509732483072              0.0000      4.120000     123  \n",
       "4  628085266293653504             -0.6249      5.437500     105  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('resources/data/train.csv')\n",
    "\n",
    "rt = r'rt'\n",
    "train['message'] = train['message'].str.lower()\n",
    "train['message'] = train['message'].replace(to_replace = rt, value = '', regex = True)\n",
    "#train['message'] = train['message'].str.replace(r'@', 'twithandle ')\n",
    "\n",
    "# Remove stop words\n",
    "def remove_stop_words(word):\n",
    "    if word not in stop:\n",
    "        return word\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def sentiment_score(sentence):\n",
    "    \n",
    "    return sid.polarity_scores(sentence)['compound']\n",
    "\n",
    "def avg_word_len(sentence):\n",
    "    \"\"\"takes in a sentence, tokenizes it and computes\n",
    "        the average word length of the sentence\n",
    "        \n",
    "        parameters:\n",
    "        -----------\n",
    "        sentence: str\n",
    "    \n",
    "        returns:\n",
    "        --------\n",
    "        avg word length: float        \n",
    "    \"\"\"  \n",
    "    word_list = word_tokenize(sentence)\n",
    "    length = 0\n",
    "    for word in word_list:\n",
    "        length += len(word)\n",
    "        \n",
    "    return length/len(word_list)\n",
    "    \n",
    "stop = stopwords.words('english')\n",
    "\n",
    "train['message'] = [' '.join([remove_stop_words(word) for word in text.split(' ')])for text in train['message']]\n",
    "train['sentiment_compound'] = train['message'].apply( sentiment_score)\n",
    "train['avg_word_len'] = train['message'].apply(avg_word_len)\n",
    "train['length'] = train['message'].apply(len)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 71.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xxx\\anaconda3\\Anaconda364bit\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(train.message, train.sentiment, test_size=0.2, random_state=2020)\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 72.87%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LinearSVC(loss = 'squared_hinge',C = 1.1))])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n",
    "\n",
    "\n",
    "#save model\n",
    "import pickle\n",
    "\n",
    "model_save_path = \"resources/models/linearSVC.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(model,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 58.52%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', MultinomialNB())])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 62.81%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', BernoulliNB())])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 52.5%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', GradientBoostingClassifier(loss = 'deviance',\n",
    "                                                   learning_rate = 0.01,\n",
    "                                                   n_estimators = 10,\n",
    "                                                   max_depth = 5,\n",
    "                                                   random_state=55))])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:21:21] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { loss } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "accuracy: 57.35%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', XGBClassifier(loss = 'deviance',\n",
    "                                                   learning_rate = 0.01,\n",
    "                                                   n_estimators = 10,\n",
    "                                                   max_depth = 5,\n",
    "                                                   random_state=2020))])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 70.9%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', SGDClassifier())])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 56.84%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = 10, \n",
    "                                           splitter='best', \n",
    "                                           random_state=2020))])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 68.09%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', RandomForestClassifier(criterion='entropy',n_estimators = 150))])\n",
    "\n",
    "model = pipe.fit(x_train, y_train)\n",
    "prediction = model.predict(x_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.message,\n",
    "Y = train.sentiment\n",
    "#le = LabelEncoder()\n",
    "#Y = le.fit_transform(Y)\n",
    "#Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(train.message, train.sentiment, test_size=0.15, random_state=2020)\n",
    "max_words = 140\n",
    "max_len = 75\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(x_train)\n",
    "sequences = tok.texts_to_sequences(x_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(512)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "model = RNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "WARNING:tensorflow:From C:\\Users\\xxx\\anaconda3\\Anaconda364bit\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model \n",
    "plot_model(model, to_file='model1.png')\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\xxx\\anaconda3\\Anaconda364bit\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 20916 samples, validate on 5229 samples\n",
      "Epoch 1/2\n",
      "20916/20916 [==============================] - 292s 14ms/step - loss: 0.2449 - accuracy: 0.4869 - val_loss: 0.2873 - val_accuracy: 0.5177\n",
      "Epoch 2/2\n",
      "20916/20916 [==============================] - 268s 13ms/step - loss: 0.5746 - accuracy: 0.5018 - val_loss: 0.1282 - val_accuracy: 0.4903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a33db26fc8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,y_train,batch_size=256,epochs=2,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4614/4614 [==============================] - 33s 7ms/step\n",
      "Accuracy: 0.49\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tok.texts_to_sequences(x_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "accr = model.evaluate(test_sequences_matrix,y_test)\n",
    "print('Accuracy: {:0.2f}'.format(accr[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
